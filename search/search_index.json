{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Holodeck 9.0","text":""},{"location":"#what-is-holodeck","title":"What is Holodeck?","text":"<p>Holodeck is a toolkit designed to provide a standardized and automated method to deploy nested VMware Cloud Foundation (VCF) environments on a VMware ESX host or a vSphere cluster. These environments are ideal for technical capability testing by multiple teams inside a data center to explore hands-on exercises showcasing VCF capabilities to deliver a customer managed VMware Private Cloud. Holodeck is only to be used for a testing and training environment; it is ideal for anyone wanting to gain a better understanding of how VCF functions across many use cases and capabilities. Currently, there are two different versions of the Holodeck supported - Holodeck 5.2x supporting VCF 5.2.x and Holodeck 9.0 supporting VCF 5.2.x and VCF 9.0. </p> <p>This documentation solely focuses on Holodeck 9.0. If you need details on the previous version of Holodeck, please refer to this documentation</p>"},{"location":"#advantages-of-holodeck","title":"Advantages of Holodeck","text":"<p>While there are multiple ways to deploy nested VCF environments, this can be time consuming and may require specific settings to ensure optimal experience. That's where Holodeck comes in. Some of the challenges Holodeck helps overcome are:</p> <ul> <li> <p>Reduced hardware requirements: When operating in a physical environment, VCF requires four vSAN Ready Nodes for the management domain, and additional hosts for adding clusters or workload domains. In a nested environment, this same four to eight hosts are easily virtualized to run on a single ESX host or a vSphere cluster.</p> </li> <li> <p>Self-contained services: Holodeck comes with built-in common infrastructure services, such as NTP, DNS, AD, Certificate Services and DHCP within the environment, removing the need to rely on data center provided services during testing. Each environment needs a single external IP.</p> </li> <li> <p>Isolated networking: Holodeck removes the need for VLAN and BGP connections in the customer network early in the testing phase.</p> </li> <li> <p>Isolation between environments: Each Holodeck deployment is completely self-contained. This avoids conflicts with existing network configurations and allows for the deployment of multiple nested environments with no concerns for overlap.</p> </li> <li> <p>Multiple VCF deployments on a single VMware ESX host of sufficient capacity: A typical VCF Standard Architecture deployment of four node management domain and three node VI workload domain requires approximately 24 CPU cores, 325GB memory and 1.1TB disk for VCF 9.0.</p> </li> <li> <p>Automation and repeatability: The deployment of a nested VCF environments is almost completely hands-off, and easily repeatable.</p> </li> </ul>"},{"location":"#holodeck-environment-overview","title":"Holodeck Environment Overview","text":"<p>Each Holodeck environment contains:</p> VCF 9.0VCF 5.2 <ul> <li>A Holorouter appliance (photon OS based) with built-in networking services such as DNS, DHCP, NTP, Proxy, dynamic routing (BGP), L2 switching and optional webtop (virtual desktop) capability</li> <li>Support for VCF and VVF deployments</li> <li>vSAN ESA and OSA support</li> <li>Support for online and offline depot with proxy for VCF Installer</li> <li>Management Domain deployed with 4 nested hosts deployed as vSAN ready nodes including VCF Installer, VMware vCenter, VMware NSX, VCF Operations, VMware SDDC Manager, VCF Automation (optional)</li> <li>Optional Workload Domain deployed with 3 nested hosts deployed as vSAN ready nodes including VMware vCenter, VMware NSX and Supervisor (optional)</li> <li>Optional NSX Edge Cluster deployment in management and/or workload domain</li> <li>Deploy one or many additional 3-node vSphere cluster in management domain</li> <li>Support for provision-only mode (deploy VCF Installer and ESX hosts to allow greenfield deployment experience)</li> <li>Custom CIDR support for Holodeck network</li> </ul> <ul> <li>A Holorouter appliance (photon OS based) with built-in networking services such as DNS, DHCP, NTP, Proxy, dynamic routing (BGP), L2 switching and optional webtop (virtual desktop) capability</li> <li>Support for VCF deployment only</li> <li>vSAN OSA support only</li> <li>Management Domain deployed with 4 nested hosts deployed as vSAN ready nodes including VMware Cloud Builder, VMware vCenter, VMware NSX, VMware SDDC-Manager</li> <li>Optional Workload Domain deployed with 3 nested hosts deployed as vSAN ready nodes including VMware vCenter and VMware NSX</li> <li>Optional NSX Edge Cluster deployment in management and/or workload domain</li> <li>Deploy one or many additional 3-node vSphere cluster in management domain</li> <li>Custom CIDR support for Holodeck network</li> </ul> <p>Note: Holodeck 9.0 is not a VMware supported product, it is similar to a Fling.</p> <p>Holodeck 9.0 supports nested VCF deployment for versions 5.2 and 9.0. This can be deployed either on a single stand-alone ESX host or a vSphere cluster based on resource availability. Please check the Pre-requisites section</p> <p>Holodeck 9.0 has been developed using PowerShell and VMware PowerCLI. We have bundled and packaged everything needed into a powershell module called Holodeck. This powershell module is provided to you as an in-built functionality within the OVA we ship called Holorouter. </p> <p>Each Holodeck environment runs an identical nested configuration. A Holodeck environment can be deployed as a Single or Dual site Configuration. Separation of the environments and between sites within an environment is handled at the  VMware vSphere Standard Switch (vSS) or VMware vSphere Distributed Switch (vDS) level. Each Holodeck pod is configured with a unique port group on the vSS/vDS per site. A VMware vSphere Port Group is configured on each vSS/vDS and configured as a VLAN trunk to facilitate communication. Components on the port group use VLAN tagging to isolate communications between nested VLANs. This removes the need to have physical VLANs plumbed to the ESX host to support nested labs. There is also an option to use an NSX overlay segment instead of a vSS/vDS port group if available.</p>"},{"location":"#holorouter-overview","title":"Holorouter Overview","text":"<p>HoloRouter is an appliance that serves as the infrastructure backbone for Holodeck. It provides infrastructure services such as Layer-3 routing, Firewall, DHCP, DNS, NTP, BGP, Proxy, Job scheduling, etc. Through these services, HoloRouter connects the nested VCF environment to the external networks. It also provides inter-connectivity between different networks in the nested VCF environment. For Site-a, VLANs 10 through 25 and for Site-b, VLANs 40 through 58 are used. It is equipped with a built-in webtop (Desktop UI) which allows users access to HoloRouter via a GUI. Through the webtop service, users get easy GUI access to the nested VCF environment. </p> <p>Scope of Services: - DNS: local to Site-a and Site-b of nested VCF environment, acts as forwarder - DHCP: local to Site-a and Site-b of nested VCF environment - NTP: local to Site-a and Site-b of nested VCF environment - L3 routing between VLANs of Site-a and Site-b of nested VCF environment - Firewall to control traffic to and from external networks and between the networks in the nested VCF environment - BGP: to establish relationship with NSX Tier-0 gateway for outbound connectivity for overlay networks - Proxy: allows users to control the outbound connectivity for the nested VCF environment - Job scheduling: allows users to schedule commands/scripts to be run recursively - Webtop: allows users to access HoloRouter and nested VCF environment via a simple GUI - Powershell with VMware PowerCLI and other associated modules: allows users to consume Holodeck  - All required packages to deploy and operate Holodeck</p>"},{"location":"#concepts","title":"Concepts","text":"<ul> <li> <p>Centralized Configuration: Holodeck 9.0 has been designed around the concept of a centralized config file that acts as the source of truth for nested VCF deployments. The config file is a JSON file with a set of templates that are needed to run Holodeck. Customers are not expected to interact with the config file directly or edit it. We have built powershell cmdlets that help create, edit or import the config file as needed. The default template for config file is stored in /holodeck-runtime/templates/config.json. This config.json file is replicated and placed in /holodeck-runtime/config/&lt;config-ID&gt;.json when a new holodeck config is created using the cmdlet New-HoloDeckConfig.</p> </li> <li> <p>Idempotency: We know that deploying an entire full stack SDDC deployment can be time consuming. We also know that this time can increase even further when performing nested deployments. We've all been in a situation where we reach towards the end of the deployment only to realize we missed something minor that causes deployment failure and we have to start all over again. To solve this challenge, we've brought in the idempotency feature in Holodeck 9.0. We store the state of the holodeck deployment on Holorouter thus allowing users to run the same command used to deploy Holodeck and pick up right where the code failed, eliminating the need to restart entire deployment or proceed manually in case of failure.</p> </li> <li> <p>Automated Networking: Assigning VLANs, IP addresses, routes etc for each of your deployments can seem like a daunting task. We take this pain away in Holodeck 9.0. We use a default CIDR (10.1.0.0/20) and build out the entire networking including DNS mapping for each of your nested hosts and VCF components, entire routing including BGP setup for NSX Edge peering. For end users looking to deploy Holodeck in a custom CIDR, we provide the option to bring in your own CIDR of /20 size as an input parameter and we automatically use that to deploy VCF in the CIDR you provide.</p> </li> <li> <p>Built-In PreChecks: Holodeck 9.0 runs a set of pre-checks when a new deployment is run to ensure everything needed is available such as all the required binaries are available in the right location or not, is the target host reachable etc.</p> </li> </ul>"},{"location":"#download-the-required-software","title":"Download the Required Software","text":"<p>Navigate to the Downloads Page to download Holodeck binaries.</p>"},{"location":"#pre-requisites","title":"Pre-requisites","text":""},{"location":"#physical-host-requirements","title":"Physical Host requirements","text":"VCF 5.2 Single Site Dual Site CPU 16 32 Memory 384 GB 1TB Disk 2 TB 4TB VCF 9.0 Single Site Dual Site CPU 24 48 Memory 325 GB 768 GB Disk 1.1 TB 2.5TB VVF 9.0 Single Site Dual Site CPU 12 24 Memory 256 GB 512 GB Disk 1 TB 2TB"},{"location":"#configuration-requirements","title":"Configuration requirements","text":"<ol> <li>         Create a dedicated trunk port on the vSwitch (vSS) or vDS for connecting to Holorouter. Dedicated port group ensures Holodeck does not interfere with your environment's networking. An NSX overlay trunk port group can be used instead as well.     </li> <li>         If vSS/vDS port group is used, enable security settings on the trunk port group as below: Figure: Security Settings for vSS Port Group Figure: Security Settings for vDS Port Group </li> <li>         If NSX port group is used, ensure the type is Overlay. Create custom segment profiles with settings as per below by navigating to Networking --&gt; Segments tab on the left navigation bar, then click on Profiles tab on the right, click on Add segment profile and select the profiles as per below          Figure: IP Discovery Profile in NSX Figure: MAC Discovery Profile in NSX Figure: Segment Security Profile in NSX          Once the profiles have been created, navigate to the overlay segment you wish to use and edit the segment and update the segment profiles association.      </li> <li>         If a vCenter is used as the target for deploying nested VCF lab, then VLANs 10 through 25 and 40 through 58 need to be allowed on the physical switches to allow inter-host communication within the vSphere cluster where the nested VCF deployment will occur.     </li> </ol>"},{"location":"#target-host-configuration","title":"Target Host Configuration","text":"<ul> <li>Version vSphere 8.0u3 and 9.0 have been tested and are supported</li> <li>Stand-alone non vCenter Server managed host or a vSphere cluster managed by a VMware vCenter server instance</li> <li>Virtual Standard switch and port groups configured per guidelines</li> <li>External/Customer networks required</li> <li>ESX host management IP (one per host)</li> <li>4 CPU, 4 GB RAM and 75 GB storage for Holorouter (Internet access is optional)</li> <li>Backing storage should be SSD/NVMe based</li> <li>Holorouter external IP address per Holodeck Environment</li> <li>NTP service needs to be enabled and an NTP server must be configured. If using vCenter as the target, then all hosts within the vCenter cluster must have NTP running and configured.</li> </ul>"},{"location":"#licensing","title":"Licensing","text":"<p>Holodeck 9.0 only supports VCF 5.2.x and 9.0 in \"License Later\" deployment mode. This mode enables all functionality for 90 days from the date of  install for VCF 9.0 and for 60 days for VCF 5.2. After that time period expires, the environment will need to be redeployed, or license must be added. Licensing is the responsibility of the end-user to ensure they procure the appropriate licenses by working with their account teams.</p>"},{"location":"#deployment","title":"Deployment","text":""},{"location":"#prepare-physical-esx-for-holodeck-networking","title":"Prepare Physical ESX for Holodeck Networking","text":"<p>Each Holodeck environment requires an isolated (no uplinks) vSphere Standard Switch and corresponding Port Groups.</p>"},{"location":"#pre-requisites_1","title":"Pre-Requisites","text":"<p>External facing Port Group configured with an IP address available for each Holodeck environment to be deployed on this host.</p>"},{"location":"#esx-host-networking-configuration","title":"ESX Host Networking Configuration","text":"<p>This task describes the process for configuring a vSwitch called Holo-PG-A and a port group called Holo-PG-A</p> <p>Note: Adding the second switch and port group for Site-2 is recommended even if you do not initially deploy the second site within the pod.</p>"},{"location":"#configure-vsphere-standard-switches-for-nested-networking","title":"Configure vSphere Standard Switches for Nested Networking","text":"<ol> <li>Create a standard switch called Holo-PG-A and MTU 8000.</li> <li>Remove the uplink by clicking on the X on the uplink.</li> <li>Verify the settings and click Add</li> </ol>"},{"location":"#configure-holodeck-port-groups","title":"Configure Holodeck Port Groups","text":"<ol> <li>Add a new Port Group</li> <li>Name the Port Group Holo-PG-A</li> <li>Set VLAN ID to 4095</li> <li>Set virtual switch to Holo-PG-A</li> <li>Open security and set all to accept</li> <li>Click Add</li> </ol>"},{"location":"#deploy-holorouter-ova","title":"Deploy Holorouter OVA","text":"<p>Holodeck 9.0 supports deployments on both stand-alone ESX hosts as well as vCenter as target. Choose the appropriate tab below to follow the instructions for your specific target to deploy Holorouter.</p> Stand-Alone ESX HostvCenter <p><ol> <li>         Log in to your target ESX host web interface \"https://\"      <li>         Right Click \"Virtual Machines\" and select \"Create/Register VM\"          </li> <li>         In the modal window select \"Deploy a virtual machine from OVF or OVA file\"          </li> <li>         Give the VM a name and select the Holorouter OVA you downloaded previously          </li> <li>         Select the appropriate Storage and Networking where your Holodeck instance will be deployed. You will select an (External) port group for Management and then another (Trunk) for Site A and Site B - With the default Site configurations you can effectively use the same port group for both sites (They are on discrete VLANs and subnets).          </li> <li>         Agree to the EULA     </li> <li>         To use DHCP leave the boxes blank, if your DHCP does not offer a DNS server, please fill that in. The other options is to statically assign a Management IP, CIDR, Gateway and DNS that will have access to the rest of your network, this IP can be used to access Holorouter and the components that will be deployed by Holodeck.          </li> <li>         Select the checkboxes for SSH and/or Webtop          NOTE If Webtop is selected you'll have a \"light\" desktop with a browser available on port 30000 of the holorouter management IP. There is no authentication so be careful not to expose this externally, or do not select this option if you do not want it exposed. </li> <li>         Click Finish     </li> <p><ol> <li>         Log in to your vCenter server https:// <li>         Right click your cluster and select \"Deploy OVF Template\"          </li> <li>         Give the VM a name and select the Holorouter OVA you downloaded previously          </li> <li>         Select the appropriate Storage and Networking where your Holodeck instance will be deployed. You will select an (External) port group for Management and then another (Trunk) for Site A and Site B - With the default Site configurations you can effectively use the same port group for both sites (They are on discrete VLANs and subnets).          </li> <li>         To use DHCP leave the boxes blank, if your DHCP does not offer a DNS server, please fill that in. The other options is to statically assign a Management IP/CIDR/GW/DNS that will have access to the rest of your network, this IP can be used to access Holorouter and the components that will be deployed by Holodeck.          </li> <li>         Select the checkboxes for SSH and/or Webtop          NOTE If Webtop is selected you'll have a \"light\" desktop with a browser available on port 30000 of the holorouter management IP. There is no authentication so be careful not to expose this externally, or do not select this option if you do not want it exposed. </li> <li>         Click Finish     </li>"},{"location":"#accessing-holodeck-environment","title":"Accessing Holodeck Environment","text":"<p>Users access to the Holodeck environment is via the Holorouter. Access to Holorouter is available via two paths:</p> <ul> <li> <p>For UI access, open a web browser from a JumpHost or Console that has access to the external IP of Holorouter and navigate to the URL http://:30000 <li> <p>For CLI access, SSH to Holorouter using the command:</p> </li> <pre><code>ssh root@&lt;Holorouter IP&gt;\n</code></pre> <p>Use the password that was set for Holorouter during OVA deployment.</p>"},{"location":"#stage-software-to-build-host","title":"Stage software to build host","text":"<p>Upload the binaries for VCF Installer/Cloud Builder and VMware ESX that were downloaded in the Pre-requisites section to the below folder on holorouter:</p> <p>For VCF 9.0: Folder = \"/holodeck-runtime/bin/9.0/\" For VCF 5.2: Folder = \"/holodeck-runtime/bin/5.2/\"  </p> <p>The files can be downloaded by accessing the Broadcom Support Portal within the webtop UI (assuming proper entitlement is available for end-user)</p> <p>Another option is to download the files locally and use scp to copy the files using the below command:</p> <p>For VCF 9.0:</p> <pre><code>scp /&lt;local-path&gt;/&lt;ESX ISO File Name&gt; root@&lt;Holorouter-IP&gt;:/holodeck-runtime/bin/9.0/\nscp /&lt;local-path&gt;/&lt;VCF Installer OVA File Name&gt; root@&lt;Holorouter-IP&gt;:/holodeck-runtime/bin/9.0/\n</code></pre> <p>For VCF 5.2:</p> <pre><code>scp /&lt;local-path&gt;/&lt;ESX ISO File Name&gt; root@&lt;Holorouter-IP&gt;:/holodeck-runtime/bin/5.2/\nscp /&lt;local-path&gt;/&lt;VCF Installer OVA File Name&gt; root@&lt;Holorouter-IP&gt;:/holodeck-runtime/bin/5.2/\n</code></pre>"},{"location":"#run-holodeck-deployment","title":"Run Holodeck deployment","text":"<p>Once logged in to Holorouter via SSH or webtop (access the CLI inside webtop), run the following commands:</p> <p>Open PowerShell:</p> <pre><code>pwsh\n</code></pre> <p>Use the command below to create a new Holodeck config. This command creates a config file specific for your deployment with a unique config ID and loads the config file into the $config variable. </p> <p><code>New-HoloDeckConfig -Description &lt;Description&gt; -TargetHost &lt;Target vCenter/ESX IP/FQDN&gt; -Username &lt;username&gt; -Password &lt;password&gt;</code></p> <p></p> <p>Multiple config files can be created by running the below command multiple times for different use-cases such as 1 config for VCF 5.2 deployment and another for VCF 9.0 deployment. </p> <p>To check which config is currently loaded in your powershell session, run the below command and check the config ID or description:</p> <pre><code>$config\n</code></pre> <p></p> <p>Note that $config is specific to a powershell session. If you exit powershell and open a new session, you will need to import the config using:</p> <pre><code>Get-HoloDeckConfig\n</code></pre> <p>The above command gives a list of config files available. Note the config ID for your specific deployment to use in the below command.</p> <pre><code>Import-HoloDeckConfig -ConfigId &lt;String&gt;\n</code></pre> <p></p> <p>The same procedure can be followed if you wish to switch from one config file to another as well. </p> <p>Deploy a Holodeck instance using the New-HoloDeckInstance command. This command can be operated in 4 different ways as shown below:</p> <p></p> <p>If you notice closely, some of the parameters have a square bracket around them while others do not. The ones that have square brackets around them are optional parameters. With this information, let's look at each option in the Syntax section of the above image.</p> <p><code>New-HoloDeckInstance -Version &lt;String&gt; [-InstanceID &lt;String&gt;] [-CIDR &lt;String[]&gt;] [-vSANMode &lt;String&gt;] [-LogLevel &lt;String&gt;] [-ProvisionOnly] -VVF [-Site &lt;String&gt;] [-DepotType &lt;String&gt;] [-DeveloperMode] [&lt;CommonParameters&gt;]</code></p> <p>In the first option, we see that -VVF and -Version are mandatory, showcasing this syntax is used for VVF deployment. </p> <p>Note: VVF deployment is supported only when -Version is selected as \"9.0\". Using -Version \"5.2\" with -VVF yields no result.</p> <p><code>New-HoloDeckInstance -Version &lt;String&gt; [-InstanceID &lt;String&gt;] [-CIDR &lt;String[]&gt;] [-vSANMode &lt;String&gt;] -ManagementOnly [-NsxEdgeClusterMgmtDomain] [-DeployVcfAutomation] [-LogLevel &lt;String&gt;] [-ProvisionOnly] [-Site &lt;String&gt;] [-DepotType &lt;String&gt;] [-DeveloperMode] [&lt;CommonParameters&gt;]</code></p> <p>In the second option, we see that -ManagementOnly and -Version is mandatory, showcasing this syntax is used to deploy a nested VCF deployment with management domain only. </p> <p><code>New-HoloDeckInstance -Version &lt;String&gt; [-InstanceID &lt;String&gt;] [-CIDR &lt;String[]&gt;] [-vSANMode &lt;String&gt;] [-WorkloadDomainType &lt;String&gt;] [-NsxEdgeClusterMgmtDomain] [-NsxEdgeClusterWkldDomain] [-DeployVcfAutomation] [-DeploySupervisor] [-LogLevel &lt;String&gt;] [-ProvisionOnly] [-Site &lt;String&gt;] [-DepotType &lt;String&gt;] [-DeveloperMode] [&lt;CommonParameters&gt;]</code></p> <p>In the third option, we see that only -Version is mandatory, but it also has an optional parameter called -WorkloadDomainType showcasing this syntax is used for deploying a full stack nested VCF deployment. -WorkloadDomainType is optional as it already has a default value set. </p> <pre><code>New-HoloDeckInstance [-Interactive] [&lt;CommonParameters&gt;]\n</code></pre> <p>The last option is used for performing day 2 activities on a Holodeck instance after it has been deployed successfully. Day 2 operations include deploying an additional vSphere cluster in the Management Domain or the Workload Domain. More day 2 capabilities to be added in future based on feedback.</p> <p>Management-Domain only deployment:</p> <p><code>New-HoloDeckInstance -Version &lt;String&gt; [-InstanceID &lt;String&gt;] [-CIDR &lt;String[]&gt;] [-vSANMode &lt;String&gt;] -ManagementOnly [-NsxEdgeClusterMgmtDomain] [-DeployVcfAutomation] [-LogLevel &lt;String&gt;] [-ProvisionOnly] [-Site &lt;String&gt;] [-DepotType &lt;String&gt;] [-DeveloperMode] [&lt;CommonParameters&gt;]</code></p> Parameter Type Required Description Options Default Value Version String Mandatory Provide VCF version \"9.0\" or \"5.2\" InstanceID String Optional Optional Instance ID used as a prefix before all nested VMs deployed as part of Holodeck to help users uniquely identify their instances. If Instance ID is not provided, a random Instance ID is generated and used. String CIDR String Optional VCF instance is deployed by default in the 10.1.0.0/20 CIDR. If you wish to use a custom CIDR, provide a CIDR of /20 size String of format: \"10.3.0.0/20\" \"10.1.0.0/20\" vSANMode String Optional Support for both vSAN Express Storage Architecture (ESA) and Original Storage Architecture (OSA) \"ESA\" or \"OSA\" \"OSA\" ManagementOnly Switch Mandatory Deploys a VCF instance with Management domain only NA NsxEdgeClusterMgmtDomain Switch Optional Deploys an NSX Edge Cluster in Management domain (AVN included if deploying VCF 5.2) NA DeployVcfAutomation Switch Optional Deploys VCF Automation. This is applicable only if -Version is set to \"9.0\". VCF Automation is not deployed by default unless this switch is used. NA ProvisionOnly Switch Optional Deploys nested ESX hosts and VCF Installer/Cloud Builder and provides JSON API specs for performing VCF deployment manually NA Site String Optional Deploy site a or b in a VCF Instance \"a\" or \"b\" \"a\" DepotType String Optional Applicable for -Version 9.0 only. Choose whether VCF Installer should use the online or offline depot to download VCF 9 components. \"Online\" or \"Offline\" \"Online\" LogLevel String Optional Set the log level you wish to view One of \"INFO\", \"DEBUG\", \"SUCCESS\", \"WARN\", \"ERROR\" \"INFO\" DeveloperMode Switch Optional Used for internal Holodeck testing. Ignore NA <p>Full-Stack deployment:</p> <p><code>New-HoloDeckInstance -Version &lt;String&gt; [-InstanceID &lt;String&gt;] [-CIDR &lt;String[]&gt;] [-vSANMode &lt;String&gt;] [-WorkloadDomainType &lt;String&gt;] [-NsxEdgeClusterMgmtDomain] [-NsxEdgeClusterWkldDomain] [-DeployVcfAutomation] [-DeploySupervisor] [-LogLevel &lt;String&gt;] [-ProvisionOnly] [-Site &lt;String&gt;] [-DepotType &lt;String&gt;] [-DeveloperMode] [&lt;CommonParameters&gt;]</code></p> Parameter Type Required Description Options Default Value Version String Mandatory Provide VCF version \"9.0\" or \"5.2\" InstanceID String Optional Optional Instance ID used as a prefix before all nested VMs deployed as part of Holodeck to help users uniquely identify their instances. If Instance ID is not provided, a random Instance ID is generated and used. String CIDR String Optional VCF instance is deployed by default in the 10.1.0.0/20 CIDR. If you wish to use a custom CIDR, provide a CIDR of /20 size String of format: \"10.3.0.0/20\" \"10.1.0.0/20\" vSANMode String Optional Support for both vSAN Express Storage Architecture (ESA) and Original Storage Architecture (OSA) \"ESA\" or \"OSA\" \"OSA\" WorkloadDomainType String Optional Choose whether you want to share the management domain SSO with workload domain or use a separate SSO (wld.sso). \"SharedSSO\" or \"IsolatedSSO\" \"\" NsxEdgeClusterMgmtDomain Switch Optional Deploys an NSX Edge Cluster in Management domain (AVN included if deploying VCF 5.2) NA NsxEdgeClusterWkldDomain Switch Optional Deploys an NSX Edge Cluster in Workload domain (AVN included if deploying VCF 5.2) NA DeployVcfAutomation Switch Optional Deploys VCF Automation. This is applicable only if -Version is set to \"9.0\". VCF Automation is not deployed by default unless this switch is used. NA DeploySupervisor Switch Optional Applicable only for VCF 9.0. Deploys Supervisor in workload domain and additional networking configuration needed to activate supervisor NA ProvisionOnly Switch Optional Deploys nested ESX hosts and VCF Installer/Cloud Builder and provides JSON API specs for performing VCF deployment manually NA Site String Optional Deploy site a or b in a VCF Instance \"a\" or \"b\" \"a\" DepotType String Optional Applicable for -Version 9.0 only. Choose whether VCF Installer should use the online or offline depot to download VCF 9 components. \"Online\" or \"Offline\" \"Online\" LogLevel String Optional Set the log level you wish to view One of \"INFO\", \"DEBUG\", \"SUCCESS\", \"WARN\", \"ERROR\" \"INFO\" DeveloperMode Switch Optional Used for internal Holodeck testing. Ignore NA <p>Dual Site deployment:</p> <pre><code>New-HoloDeckNetworkConfig -Site a -MasterCIDR &lt;string&gt;\nNew-HoloDeckNetworkConfig -Site b -MasterCIDR &lt;string&gt;\nSet-HoloRouter -dualsite\nNew-HoloDeckInstance -Site a [Additional Parameters]\n</code></pre> <p>Open a new tab in powershell, import the config and run </p> <pre><code>New-HoloDeckInstance -Site b [Additional Parameters]\n</code></pre> <p>Approx times for tested workflows (for 9.0):</p> Parameters Time Notes -ManagementOnly 4-5 hours Just 4 hosts for management domain -NsxEdgeClusterMgmtDomain 5-6 hours 4 hosts without and 2 node Edge Cluster -DeployVCFAuto -DeploySupervisor 12+ 4 hosts management with VCF Automation, Supervisor implies 3 node WLD, Edges and Supervisor <p>Please note that the time mentioned above is only an indication as the actual time taken depends on multiple factors such as the physical environment, networking connectivity etc.</p>"},{"location":"#during-deployment","title":"During Deployment","text":"<p>You may see \"errors\" during the deployment phase, if they are not displayed in RED text and exit the script, they are handled and you shouldn't need to worry about them.</p>"},{"location":"#post-deployment","title":"Post Deployment","text":"<p>Once Holodeck is deployed, you can access the VCF components on your browser (local based on your networking setup or webtop):</p> Appliance FQDN Username Password Management Domain VCF Installer or Cloud Builder https://vcfinstaller-a.site-a.vcf.lab admin@local for VCF Installer admin for Cloud Builder VMware123!VMware123! VCF Operations https://ops-a.site-a.vcf.lab/ admin or admin@local VMware123!VMware123! VCF Automation https://auto-a.site-a.vcf.lab/  Organization: system admin VMware123!VMware123! ESX https://esx-01a.site-a.vcf.lab https://esx-02a.site-a.vcf.lab https://esx-03a.site-a.vcf.lab https://esx-04a.site-a.vcf.lab root VMware123!VMware123! Management vCenter https://vc-mgmt-a.site-a.vcf.lab/ administrator@vsphere.local VMware123!VMware123! SDDC-Manager https://sddcmanager-a.site-a.vcf.lab/ administrator@vsphere.local VMware123!VMware123! Management NSX https://nsx-mgmt-a.site-a.vcf.lab admin VMware123!VMware123! Workload Domain Workload vCenter https://vc-wld01-a.site-a.vcf.lab/ administrator@wld.sso if Isolated SSO enabled administrator@vsphere.local otherwise VMware123!VMware123! Workload NSX https://nsx-wld01-a.site-a.vcf.lab/ admin VMware123!VMware123! ESX https://esx-05a.site-a.vcf.lab https://esx-06a.site-a.vcf.lab https://esx-07a.site-a.vcf.lab root VMware123!VMware123! <p>The above table has been generated for Site A. If you have deployed Site B, replace \"site-a\" in the FQDN with \"site-b\". For example, Management vCenter for Site A is https://vc-mgmt-a.site-a.vcf.lab/ and Management vCenter for Site B is https://vc-mgmt-a.site-b.vcf.lab/</p>"},{"location":"#how-to","title":"How To","text":""},{"location":"#start-and-stop-holodeck-instance","title":"Start and Stop Holodeck Instance","text":"<p>There may be situations where you have already deployed Holodeck but need the resources for another operation. In that case, we provide cmdlets to power off Holodeck and power it back on as well.</p> <p>For powering off Holodeck:</p> <pre><code>Stop-HoloDeckInstance [-InstanceID] &lt;string&gt; [-Force]\n</code></pre> <p>Stop-HoloDeckInstance asks for confirmation which can be bypassed by using the -Force parameter.</p> <p>For powering on Holodeck:</p> <pre><code>Start-HoloDeckInstance [-InstanceID] &lt;string&gt; [-Force]\n</code></pre>"},{"location":"#create-new-nested-esx-hosts","title":"Create new nested ESX hosts","text":"<p>You can dynamically add new ESX hosts to an existing site using the New-HoloDeckESXiNodes cmdlet.</p> <pre><code>New-HoloDeckESXiNodes -Nodes &lt;No. of Nodes&gt; -CPU &lt;No. of vCPU&gt; -MemoryInGb &lt;Memory in GB&gt; -Site &lt;'a' or 'b'&gt; -vSANMode &lt;'ESA' or 'OSA'&gt;\n</code></pre>"},{"location":"#remove-nested-vcf-instance","title":"Remove nested VCF Instance","text":"<p>To remove a HoloDeck Instance, run:</p> <pre><code>Remove-HoloDeckInstance [-ResetHoloRouter]\n</code></pre> <p>Remove-HoloDeckInstance will delete al the nested ESX hosts and VCF Installer/Cloud Builder VM associated to a specific instance.</p> <p>-ResetHoloRouter will remove the networking configuration setup for the nested VCF instance. When you run New-HoloDeckInstance, the networking is configured again. This involves an automatic reboot of the Holorouter.</p>"},{"location":"#get-subnets-in-holodeck","title":"Get subnets in Holodeck","text":"<p>You can list all the subnets that have been configured or reserved for HoloDeck. You can also get information about specific subnets by specifying the name, VLAN ID, Subnet range, and Gateway IP.</p> <pre><code>Get-HoloDeckSubnet [-Name &lt;string&gt;] [-vlanID &lt;string&gt;] [-Subnet &lt;string&gt;] [-Gateway &lt;string&gt;] [-Site &lt;string&gt;] [&lt;CommonParameters&gt;]\n\nFor e.g.:\nFor Site 'a':\nGet-HoloDeckSubnet -Site a | ft -AutoSize\n\nFor Site 'b':\nGet-HoloDeckSubnet -Site b | ft -AutoSize\n\nGet-HoloDeckSubnet -Site a -Name Untagged-HOL\n\nGet-HoloDeckSubnet -Site b -vlanID 50\n\nGet-HoloDeckSubnet -Site a -Gateway 10.1.1.1\n\nGet-HoloDeckSubnet -Site a -Subnet 10.1.2.0/25\n</code></pre>"},{"location":"#get-appliance-details","title":"Get appliance details","text":"<p>You can list all the IP-Hostname entries generated by the Network Manager for HoloDeck. You can also get information about specific IP-Hostname mappings by specifying the IP, hostname, and FQDN.</p> <pre><code>Get-HoloDeckAppNetwork [-Hostname &lt;string&gt;] [-IP &lt;string&gt;] [-FQDN &lt;string&gt;] [-Site &lt;string&gt;] [&lt;CommonParameters&gt;]\n\nFor e.g.:\nFor Site 'a':\nGet-HoloDeckAppNetwork -Site a\n\nFor Site 'b':\nGet-HoloDeckAppNetwork -Site b\n\nGet-HoloDeckAppNetwork -Site a -Hostname router\n\nGet-HoloDeckAppNetwork -Site a -IP 10.1.1.10\n\nGet-HoloDeckAppNetwork -Site a -FQDN esx-01a.site-a.vcf.lab\n</code></pre>"},{"location":"#get-bgp-configuration","title":"Get BGP configuration","text":"<p>You can list the BGP configuration generated by the Network Manager for HoloDeck. </p> <pre><code>Get-HoloDeckBGPConfig [-Site &lt;string&gt;] [&lt;CommonParameters&gt;]\n\nFor e.g.:\nFor Site 'a':\nGet-HoloDeckBGPConfig -Site a\n\nFor Site 'b':\nGet-HoloDeckBGPConfig -Site b\n</code></pre>"},{"location":"#get-dns-entries","title":"Get DNS entries","text":"<p>You can list all the DNS entries configured in the DNS service in HoloRouter. You can also get information about a specific DNS entry by specificing its IP or FQDN. </p> <pre><code>Get-HoloDeckDNSConfig -ConfigPath &lt;string&gt; [-IP &lt;string&gt;] [-FQDN &lt;string&gt;] [&lt;CommonParameters&gt;]\n\nFor e.g.:\nGet-HoloDeckDNSConfig -ConfigPath $config.ConfigPath \n\nGet-HoloDeckDNSConfig -ConfigPath $config.ConfigPath -IP 10.1.1.1\n\nGet-HoloDeckDNSConfig -ConfigPath $config.ConfigPath -FQDN esx-02a.site-a.vcf.lab \n</code></pre>"},{"location":"#add-or-update-dns-entries","title":"Add or Update DNS entries","text":"<p>You can configure additional DNS entries to the DNS service in HoloRouter. To do that, use the Set-HoloDeckDNSConfig cmdlet. Note that you must specify the DNS entry in single quotes (''). <pre><code>Set-HoloDeckDNSConfig -ConfigPath &lt;string&gt; -DNSRecord &lt;string&gt; [&lt;CommonParameters&gt;]\n\nFor e.g., to create a DNS entry for '10.1.1.201 harbor.site-a.vcf.lab', you would run -\nSet-HoloDeckDNSConfig -ConfigPath $config.ConfigPath -DNSRecord '10.1.1.201 harbor.site-a.vcf.lab'\n</code></pre> <p>You can also replace the DNS entries existing in the DNS service in HoloRouter. You will still use the Set-HoloDeckDNSConfig but specify different parameters. Note that the DNS entries to be searched and replaced must be specified in single quotes (''). <pre><code>Set-HoloDeckDNSConfig -ConfigPath &lt;string&gt; -SearchDNSRecord &lt;string&gt; -ReplaceDNSRecord &lt;string&gt; [-update] [&lt;CommonParameters&gt;]\n\nFor e.g., to replace the DNS entry '10.1.1.201 harbor.site-a.vcf.lab' with '10.1.1.210 harbor.site-a.vcf.lab', you would run -\nSet-HoloDeckDNSConfig -ConfigPath $config.ConfigPath -SearchDNSRecord '10.1.1.201 harbor.site-a.vcf.lab' -ReplaceDNSRecord '10.1.1.210 harbor.site-a.vcf.lab'\n</code></pre>"},{"location":"#remove-dns-entries","title":"Remove DNS entries","text":"<p>You can remove the DNS entries from the DNS service in HoloRouter. To do that, use Remove-HoloDeckDNSConfig cmdlet. You must specify the DNS entry in single quotes (''). <pre><code>Remove-HoloDeckDNSConfig -ConfigPath &lt;string&gt; -DNSRecord &lt;string&gt;  [&lt;CommonParameters&gt;]\n\nFor e.g., to remove the DNS entry '10.1.1.210 harbor.site-a.vcf.lab', you would run -\nRemove-HoloDeckDNSConfig -ConfigPath $config.ConfigPath -DNSRecord '10.1.1.210 harbor.site-a.vcf.lab'\n</code></pre>"},{"location":"#troubleshooting","title":"Troubleshooting","text":"<p>Refer to the Troubleshooting section on the FAQ page for more details.</p>"},{"location":"cmd_reference/","title":"Command Reference","text":""},{"location":"cmd_reference/#new-holodeckinstance","title":"New-HoloDeckInstance","text":"<p>Creates a new HoloDeck instance \u2014 a nested VCF lab as a proof of concept for VCF.</p> <pre><code>New-HoloDeckInstance -Version &lt;String&gt; [-InstanceID &lt;String&gt;] [-CIDR &lt;String[]&gt;] [-vSANMode &lt;String&gt;] [-LogLevel &lt;String&gt;] [-ProvisionOnly] -VVF [-Site &lt;String&gt;] [-DepotType &lt;String&gt;] [-DeveloperMode]\n\nNew-HoloDeckInstance -Version &lt;String&gt; [-InstanceID &lt;String&gt;] [-CIDR &lt;String[]&gt;] [-vSANMode &lt;String&gt;] -ManagementOnly [-NsxEdgeClusterMgmtDomain] [-DeployVcfAutomation] [-LogLevel &lt;String&gt;] [-ProvisionOnly] [-Site &lt;String&gt;] [-DepotType &lt;String&gt;] [-DeveloperMode]\n\nNew-HoloDeckInstance -Version &lt;String&gt; [-InstanceID &lt;String&gt;] [-CIDR &lt;String[]&gt;] [-vSANMode &lt;String&gt;] [-WorkloadDomainType &lt;String&gt;] [-NsxEdgeClusterMgmtDomain] [-NsxEdgeClusterWkldDomain] [-DeployVcfAutomation] [-DeploySupervisor] [-LogLevel &lt;String&gt;] [-ProvisionOnly] [-Site &lt;String&gt;] [-DepotType &lt;String&gt;] [-DeveloperMode]\n\nNew-HoloDeckInstance [-Interactive]\n</code></pre>"},{"location":"cmd_reference/#description","title":"Description","text":"<p>Deploys a HoloDeck instance based on the provided VCF version and optional parameters to customize the environment for management/workload domain, NSX Edge, vSAN mode, and more.</p>"},{"location":"cmd_reference/#parameters","title":"Parameters","text":"Name Description Required Default <code>-Version</code> VCF version. Valid: <code>\"9.0\"</code> or <code>\"5.2\"</code> \u2705 <code>-InstanceID</code> Optional prefix for all nested VMs \u274c Random <code>-CIDR</code> Custom /20 CIDR block (e.g., <code>\"10.3.0.0/20\"</code>) \u274c <code>10.1.0.0/20</code> <code>-vSANMode</code> vSAN type: <code>\"ESA\"</code> or <code>\"OSA\"</code> \u274c <code>OSA</code> <code>-ManagementOnly</code> Deploy only Management domain \u274c <code>False</code> <code>-WorkloadDomainType</code> <code>\"SharedSSO\"</code> or <code>\"IsolatedSSO\"</code> (VCF 5.2 only) \u274c <code>SharedSSO</code> <code>-NsxEdgeClusterMgmtDomain</code> Deploy NSX Edge in management domain \u274c <code>False</code> <code>-NsxEdgeClusterWkldDomain</code> Deploy NSX Edge in workload domain \u274c <code>False</code> <code>-DeployVcfAutomation</code> Deploy VCF Automation (VCF 9.0 only) \u274c <code>False</code> <code>-DeploySupervisor</code> Deploy Supervisor (VCF 9.0 only) \u274c <code>False</code> <code>-Interactive</code> Launch interactive mode for Day 2 ops \u274c <code>False</code> <code>-LogLevel</code> Log verbosity: <code>\"INFO\"</code>, <code>\"DEBUG\"</code>, etc. \u274c <code>INFO</code> <code>-ProvisionOnly</code> Provision ESX &amp; CloudBuilder only \u274c <code>False</code> <code>-VVF</code> Deploys a VVF instance \u2705 <code>-Site</code> Site to deploy: <code>\"a\"</code> or <code>\"b\"</code> \u274c <code>a</code> <code>-DepotType</code> For VCF 9.0: <code>\"Online\"</code> or <code>\"Offline\"</code> \u274c <code>Online</code> <code>-DeveloperMode</code> Internal use only \u274c <code>False</code>"},{"location":"cmd_reference/#examples","title":"Examples","text":"<p>Example 1</p> <p>Deploys a VVF using 9.0 version with vSAN ESA mode using default CIDR 10.1.0.0/20 and a randomly generated Instance ID <pre><code>New-HoloDeckInstance -Version 9.0 -vSANMode ESA -VVF -DepotType Online\n</code></pre></p> <p>Example 2</p> <p>Deploys a VCF 9.0 management domain with instance ID \"holo\" using a custom CIDR 10.3.0.0/20 with vSAN OSA and uses offline depot for VCF Installer. <pre><code>New-HoloDeckInstance -Version 9.0 -InstanceID holo -CIDR 10.3.0.0/20 -vSANMode OSA -ManagementOnly -NsxEdgeClusterMgmtDomain -DeployVcfAutomation -DepotType Offline\n</code></pre></p> <p>Example 3</p> <p>Deploys nested ESX hosts for management domain and VCF Installer and creates scripts available in /holodeck-runtime/specs/ folder to manually walk-through greenfield VCF deployment. <pre><code>New-HoloDeckInstance -Version 9.0 -InstanceID holo -CIDR 10.3.0.0/20 -vSANMode OSA -ManagementOnly -NsxEdgeClusterMgmtDomain -DeployVcfAutomation -ProvisionOnly\n</code></pre></p> <p>Example 4</p> <p>Deploys a VCF 9.0 full stack instance with NSX Edge cluster deployed in both management and workload domain, VCF Automation deployed in Management domain, supervisor deployed in workload domain using an online depot. <pre><code>New-HoloDeckInstance -Version 9.0 -NsxEdgeClusterMgmtDomain -NsxEdgeClusterWkldDomain -DeployVcfAutomation -DeploySupervisor -DepotType Online\n</code></pre></p> <p>Example 5</p> <p>Deploy additional cluster in management domain or workload domain after VCF instance has been deployed. <pre><code>New-HoloDeckInstance -Interactive\n</code></pre></p>"},{"location":"downloads/","title":"Downloads","text":"Downloads"},{"location":"downloads/#what-youll-need","title":"What You'll Need","text":"<p>To set up and deploy Holodeck 9.0, download the following components:</p>"},{"location":"downloads/#1-holorouter-ova","title":"1. HoloRouter OVA","text":"<p>Download the base appliance to run Holodeck: Download HoloRouter OVA</p> <p>Note: Ensure you're signed in to the Broadcom Support Portal with an entitled account.</p>"},{"location":"downloads/#2-vcf-binaries","title":"2. VCF Binaries","text":"<p>Download the necessary VCF 9.0 installation components (ESX, Cloud Builder / VCF Installer) from Broadcom:</p> <p>Broadcom Support Portal \u2013 VMware Cloud Foundation</p> <p>Requires a Broadcom account with VCF entitlement.</p>"},{"location":"downloads/#3-offline-depot-optional","title":"3. Offline Depot (Optional)","text":"<p>For air-gapped or internet-restricted environments:</p> <p>Download the Offline Depot OVA</p>"},{"location":"downloads/#getting-started","title":"Getting Started","text":"<p>Once you've downloaded all components, head over to the Getting Started tab for step-by-step instructions.</p>"},{"location":"downloads/#need-help","title":"Need Help?","text":"<p>Visit the Support Page to report issues or join the VCF community for discussions.</p>"},{"location":"faq/","title":"FAQ","text":"\u2753 Frequently Asked Questions (FAQ) <p>Welcome! Here are answers to some of the most common questions about Holodeck and deploying VCF 9.0.</p>"},{"location":"faq/#getting-started","title":"Getting Started","text":"What is Holodeck? <p>Holodeck is a validated reference architecture and toolkit that simplifies the setup of VMware Cloud Foundation (VCF) lab environments. It includes pre-built components like HoloRouter, pre-configured networking, and deployment scripts to accelerate VCF evaluation and training.</p> What version of VCF is supported? <p>Holodeck currently supports VCF 9.0 and VC 5.2.</p> Is internet access required for deployment? <p>Holodeck can be used in both connected and air-gapped environments. For offline deployments, you must download the Offline Depot OVA in advance.</p>"},{"location":"faq/#installation-setup","title":"Installation &amp; Setup","text":"What are the system requirements for running Holodeck? <p>Refer to the Pre-requisites section for details.</p> Do I need a license to use VCF 9.0? <p>VCF 9.0 provides a 90-day trial post which customers are required to apply license.</p>"},{"location":"faq/#troubleshooting","title":"Troubleshooting","text":""},{"location":"faq/#general-issues","title":"General Issues","text":"Where can I report a bug or issue? <p>You can open an issue on GitHub: \ud83d\udd17 vmware/Holodeck \u2013 GitHub Issues</p> I can't access the Broadcom Support Portal downloads. <ul> <li>Ensure you're signed in with a Broadcom account.</li> <li>Your account must have entitlement for VCF 9.0.</li> <li>Contact Broadcom support if access problems persist.</li> </ul>"},{"location":"faq/#pre-check-issues","title":"Pre-check Issues","text":"I'm getting <code>&lt;Component&gt; binary was not found. Download the binary from the Broadcom support portal and place it in /holodeck-runtime/bin/&lt;version&gt; folder</code>. What should I do? <p>This error occurs when the ESX and VCF Installer/Cloud Builder ISO/OVA are not placed in the required folder /holodeck-runtime/bin/&lt;9.0 or 5.2&gt;/</p> <p>Ensure that the binaries are present in the folder.</p> I'm getting <code>[ERROR] No trunk port groups found on the connected server</code>. What should I do? <p>This error appears when running the Holodeck pre-checks script, and indicates that no trunk port group was found on the target host:</p> <pre><code>PreChecks[xxxxx]: [ERROR] No trunk port groups found on the connected server. Please create a trunk port group on the Target Host and run the pre-checks script again. Exiting\n</code></pre> <p>What it means: - The pre-check process is validating the ESX host's networking setup. - A required trunk port group (i.e., one that accepts VLAN ID <code>4095</code> or multiple VLANs) does not exist on the host.</p> <p>How to fix:</p> <p>If using stand-alone ESX as the target:</p> <ol> <li>Log in to the vSphere Host Client for the ESX host </li> <li>Go to Networking \u2192 Port Groups</li> <li>Create a new port group (or edit an existing one) with the following:</li> <li>VLAN ID: <code>4095</code> (this enables VLAN trunking)</li> <li>Virtual Switch: Usually <code>vSwitch0</code> or the one attached to your uplink</li> <li>Promiscuous Mode: Accept</li> <li>MAC Address Changes: Accept</li> <li>Forged Transmits: Accept</li> <li>Save the settings and rerun the pre-checks script</li> </ol> <p>If using vCenter as the target, then perform the same steps but for the vDS.</p> <p>Why this matters: Holodeck relies on a trunk port group to simulate physical networks and allow nested components (like NSX and VCF VMs) to communicate across VLANs.</p> <p>\ud83d\udee0\ufe0f Example port group config: - Name: <code>Trunk-PG</code> - VLAN ID: <code>4095</code> - Switch: <code>vSwitch0</code></p> <p>If you're unsure how to create this, refer to VMware's documentation or the Holodeck deployment guide.</p> I'm getting <code>[ERROR] NTP service is not running on target host</code>. How do I fix this? <p>During the pre-checks phase, you may encounter this error:</p> <pre><code>PreChecks[xxxxx]: [ERROR] NTP service is not running on target host 10.162.11.248. Enable NTP service before proceeding\n</code></pre> <p>What it means: - The Network Time Protocol (NTP) service on the ESX host is not active. - Accurate and synchronized time is required across all hosts in a VCF deployment to avoid certificate, authentication, and cluster issues.</p> <p>How to fix: 1. Log in to the ESX host UI (e.g., <code>https://10.162.11.248</code>) 2. Navigate to: Manage \u2192 System \u2192 Time &amp; Date 3. Click Edit Settings    - Set time synchronization to NTP    - Add a known NTP server (e.g. <code>pool.ntp.org</code> or your internal time server) 4. Save the settings 5. Start the NTP service:    - Under the Time &amp; Date tab, click Start next to the NTP service</p> <p>CLI Option (Advanced):</p> <p>SSH into the host and run:</p> <pre><code>esxcli network firewall ruleset set -e true -r ntp\nesxcli system ntp set -s pool.ntp.org\nesxcli system ntp set -e true\nesxcli system ntp start\n</code></pre> <p>After enabling NTP, rerun the pre-checks script.</p> <p>Important: All hosts should use the same time source to avoid clock drift across the VCF environment.</p>"},{"location":"faq/#esx-build-issues","title":"ESX Build Issues","text":"I see <code>SSH service on &lt;IP&gt; is not responding</code> and <code>WARNING: Host key is not being verified since Force switch is used</code> during the ESX build process. What does it mean? <p>This log output typically appears during the host validation step:</p> <pre><code>WARNING: Host key is not being verified since Force switch is used.\nCustomIso[xxxxx]: [INFO] SSH service on 10.3.1.101 is not responding, Trying after 10 seconds\n</code></pre> <p>What it means: - The deployment script is attempting to connect to the ESX host via SSH. - It\u2019s retrying because the host is still booting or SSH is not yet available. - This is expected behavior and can take around 10 minutes for the ESX to be ready.</p> <p>What to do: - Wait for 10-15 minutes \u2014 SSH usually becomes available shortly after ESX finishes booting. - If the message persists for more than 15 minutes, confirm that:   - The ESX VM is powered on and reachable   - Networking is properly configured in your Holodeck setup</p> <p>Still stuck? Try manually SSHing into the host to confirm access:</p> <pre><code>ssh root@10.3.1.101\n</code></pre> <p>If it fails, the host may not have finished booting or networking might be misconfigured.</p>"},{"location":"faq/#miscellaneous","title":"Miscellaneous","text":"I don't have enough memory on my physical server. Can I enable memory tiering and still deploy Holodeck? <p>while memory tiering is great for optimized performance and resource consumption in case of application workloads, it is not recommended when running nested environments,  it is not recommended when running nested environments like Holodeck. These environments are sensitive to latency and memory access speeds, and memory tiering can introduce  significant performance degradation.</p> <p>Although the deployment might technically complete, you may experience: - Crashes or instability in nested ESX hosts - Prolonged deployment times - Overall sluggish performance</p> <p>\ud83d\udc49 Recommendation: Only deploy Holodeck on systems with sufficient physical RAM and without memory tiering enabled to ensure a stable and responsive experience.</p> I deployed VCF in vSAN ESA mode with large disks, but the datastore appears fully consumed. Why? <p>When using vSAN ESA (Express Storage Architecture) in a nested setup, it's common to see high disk usage even when the documented minimum requirements are met.</p> <p>\ud83d\udd0d Why this happens: - ESA is designed for high-throughput, direct-attached NVMe or vSAN-backed storage. - In a nested environment, performance and storage efficiency are reduced\u2014especially if the underlying physical storage is not vSAN.</p> <p>\ud83d\udeab If you're running nested vSAN ESA on non-vSAN storage (e.g. local disks or NFS), you may encounter: - Unexpected datastore consumption - Poor performance or storage overhead - Inefficient space usage due to thin provisioning at multiple layers</p> <p>\u2705 Recommendation: For optimal results, deploy Holodeck and nested vSAN ESA on physical infrastructure that is also running vSAN.</p>"},{"location":"faq/#community-resources","title":"Community &amp; Resources","text":"Where can I get help or connect with others? <ul> <li>Join the Broadcom VCF Community: VMware Holodeck Community</li> <li>Open a GitHub discussion or issue</li> <li>Stay tuned for webinars and virtual workshops</li> </ul>"},{"location":"faq/#downloads","title":"Downloads","text":"Where can I find the download links? <p>Visit the Downloads Page for all required OVAs, installers, and optional offline tools.</p> <p>Need more help? Visit the Support Page or submit a an issue via GitHub</p>"},{"location":"offline_depot/","title":"Offline Depot","text":"Offline Depot"},{"location":"offline_depot/#overview","title":"Overview","text":"<p>The Offline Depot Appliance (ODA) is intended to help with the creation and maintenance of a VCF offline depot. A offline depot provides software for installation or updates to one or more VMware Cloud Foundation instances.</p> <p>An offline depot is not a requirement for VMware Cloud Foundation, as it has the out-of-the-box ability to leverage a online depot. As the online depot is managed by Broadcom, it has all the binaries accessible for a customer's entitlements. In contrast, a offline depot is managed by a customer and can be configured to only have specific binaries (if desired). </p> <p>However, a offline depot has several advantages: - It can provide organizations with ability to use a offline depot in environments where external network connectivity is restricted or unavailable. - It can provide faster download and installation speeds, as the offline depot would be local to the organization.  - It allows organizations to curate the binaries that they wish to make available to their VCF Instances.</p> <p>In addition, this Offline Depot Appliance provides a couple of additional features over and beyond simple offline depot:</p> <ul> <li> <p>It supports Holodeck Holodeck is a tool that automates the deployment of a virtualized VMware Cloud Foundation instance. This can be helpful to organizations in testing integrations, APIs, and general functionality as it requires a significantly reduced physical hardware footprint (and cost). </p> </li> <li> <p>It provides for easy management of the depot The user can optionally enable a Jupyter Lab instance on the depot. In doing so, a couple of Jupyter notebooks are included which help perform various tasks related to depot maintenance and Holodeck integration. </p> </li> </ul>"},{"location":"offline_depot/#obtaining-a-download-token","title":"Obtaining a Download Token","text":"<p>In order to leverage the ODA to download and populate the depot with the required binaries, you will need a download token. </p> <p>You can obtain a download token by following the instructions described in this Broadcom KnowledgeBase Article.</p>"},{"location":"offline_depot/#deployment-options-if-using-holodeck","title":"Deployment Options if Using Holodeck","text":"<p>If you are using Holodeck, you basically have two options for where to deploy the ODA: </p> <ul> <li>It can be deployed on the management network</li> <li>It can be deployed on the isolated network.</li> </ul> <p>The recommended deployment method is to install the ODA on the management network. This allows the ODA to have network access to download the bits as well as to allow the AI chatbot to function. </p> <p>With this type of deployment, you can utilize the included Jupyter notebooks to perform several of the tasks needed when using Holodeck. This includes things like copying the binaries to the Holorouter as well as performing common tasks, such as getting information from the HoloDeck configuration.  </p> <p>The other method is to install the ODA on the isolated network. Although this works, there are a few more steps required.</p> <p>As the isolated network doesn't have network access until the Holorouter is all configured with BGP and DNS, you can not download the binaries to the ODA.</p> <p>This means that you would have to manually download and copy the SDDC Manager and ESX binaries to the Holorouter and start the Holorouter deployment.</p> <p>Once the Holorouter is up and online, then it can forward requests out to the internet and this allows you to start performing the binary downloads directly to the ODA as the ODA will now have internet access.</p>"},{"location":"offline_depot/#deploying-the-offline-depot-appliance","title":"Deploying the Offline Depot Appliance","text":"<p>To deploy the appliance, simply deploy the OVA on your vCenter Server like you would normally do.</p> <p>Next, You will provide the VM a name and specify the location where it will be deployed to:</p> <p>Next, you will select the compute resource to deploy the VM to. Note that you can check the box so that the VM is automatically started after it is finished being imported. If you wanted to watch the first boot process, then you can leave this unselected and manually power on the VM.</p> <p>Next, you see the review page - Just click Next to continue.</p> <p>Next, you will need to select the appropriate storage. Ensure that the storage has enough space for your needs. </p> <p>At the next screen, you can select the network that you want to deploy the ODA to. Please see the above section that talks about the different ways you could deploy the appliance. In short though, it is recommended that you put the appliance on the management network.</p> <p>Now you've gotten to the point where you will need to specify various networking attributes, such as the host name, IP address, and Net Mask. Use the values required for your environment.</p> <p>The next section of attributes are used to set the password for the admin user. This is the user account that you will use to SSH into the appliance.</p> <p>After this, you can select some options specific to the depot configuration. </p> <ul> <li>Skip Binary Download If you select this, the appliance will not attempt to automatically download the required VCF binaries when it powers on. This is very handy if you do not have your download token at the time you are installing the appliance. </li> </ul> <p>After the appliance is deployed, you can either manually populate the depot or you can utilize the automation provided in the included Jupyter Notebooks to help you populate the depot. </p> <ul> <li>Download Token You will need to provide this in order to the appliance to attempt to automatically download the VCF binaries. If you do not provide this, then the appliance will attempt to download the required bits and then fail after timing out. </li> </ul> <p>As in the case if you selected to skip the binary download, you can always manually populate the depot or use the Jupyter Notebooks later.</p> <ul> <li>VCF Version This specifies the VCF version you wish to have the appliance download the binaries for. For VCF 9, simply enter '9.0' here.</li> </ul> <p>The Advanced section provides you the ability to enable SSH to the appliance as well as the option to enable the Jupyter Lab server. It is highly recommended that you enable the Jupyter Lab server and this option will be checked by default. </p>"},{"location":"offline_depot/#initial-boot","title":"Initial Boot","text":"<p>After you deploy the appliance, you'll want to power it on. Note that the first time you power it on, it will perform some configuration steps and then reboot itself. These configuration steps will only occur the first time you boot the VM.</p> <p>Please wait until the second boot completes until trying to use the appliance.</p>"},{"location":"offline_depot/#accessing-the-appliance-web-server","title":"Accessing the Appliance Web Server","text":"<p>With the appliance online, use a web browser and go to:</p> <p><code>http://&lt;ODA_IP&gt;</code></p> <p>You should see this if you selected the option to skip the automatic download of the binaries or if the appliance had some issue trying to download the binaries:</p> <p>This indicates that no binaries have been put into place on the depot. After you download binaries (see below) then this will be populated.</p>"},{"location":"offline_depot/#accessing-the-jupyter-lab-server","title":"Accessing the Jupyter Lab Server","text":"<p>You can also access the Jupyter Lab server (if you enabled it) by using the following URL: </p> <p>http://:8888 <p>Here, you will see two Jupyter notebooks that can assist you in performing a variety of tasks. </p>"},{"location":"offline_depot/#accessing-the-vcf-product-documentation","title":"Accessing the VCF Product Documentation","text":"<p>To make things a bit easier for people, copies of the VCF 5.2 and 9.0 product documentation is included on the appliance under /var/www/docs</p>"},{"location":"offline_depot/#logging-into-the-oda","title":"Logging into the ODA","text":"<p>You can login to the ODA appliance as the user admin with the password you set at boot.</p> <p>If you need to become root, simply use sudo</p> <pre><code>sudo su\n</code></pre>"},{"location":"offline_depot/#populating-the-binaries","title":"Populating the binaries","text":"<p>There are two main methods you can do this by. These are listed below.</p>"},{"location":"offline_depot/#option-one-leverage-the-jupyter-notebook","title":"Option One: Leverage the Jupyter Notebook","text":"<p>This is the preferred method, as it makes things a bit easier. Simply access the Depot Maintenance Jupyter Notebook and go to the section about downloading the binaries and follow the directions.</p> <p>Please note that the Jupyter Notebook will display the output of the command cell executed (if there is any). For example, if you ran the cell to download the ESX binaries for Holodeck, you would see something similar to this:</p> <p>It's important to note that you can make these Jupyter Notebooks specific to your environment by modifying the commands or adding more.</p>"},{"location":"offline_depot/#option-two-manually-use-the-vcf-download-tool","title":"Option Two: Manually Use the VCF Download Tool","text":"<p>The VCF Download Tool (VDT) is also included with this version of the ODA under the /root/vdt/bin directory.</p> <p>The VDT tool replaces the Offline Bundle Transfer Utility (OBTU) tool previously used. It also has some extensive help that you can access by using the -help argument.</p>"},{"location":"offline_depot/#directory-structure-and-permissions","title":"Directory Structure and Permissions","text":"<p>If you populate the binaries manually, then you need to ensure the files are in the proper location with the proper permissions. </p> <p>Again, to assist you with this, you can leverage the Jupyter Notebooks or you can manually execute the commands:</p> <p>Failure to set the permissions properly will result in the inability to download the files from the depot.</p> <p>The directory structure varies depending on what version of VCF you populated the depot with. For VCF 9.0, you will see that under the root directory, you'll have a directory called PROD. Under this, you'll see a directory called COMP and here you will see all the various binaries used.</p> <p>You will also see a directory for metadata and vsan under /PROD as well. If you are populating the binaries manually, ensure these files exist.</p>"},{"location":"offline_depot/#caveats","title":"Caveats","text":"<p>There are a few things that you should be aware when using this depot:</p> <ul> <li> <p>The depot does not use HTTPS. Doing so would require the generation and maintenance of certs. This just adds complexity to everything that isn't needed for most lab environments. </p> <ul> <li>You will need to change the application-prod.properties file attributes to allow for the use of http.</li> <li>You will likely see a error in the VCF Installer UI saying that you have to use HTTPS. If you've made all the changes, then you can ignore this warning.</li> <li>If you are using Holodeck, it automatically will make the required changes for you.</li> </ul> </li> <li> <p>The disk size is only about 300g for the appliance. Depending on what you are doing, you might need more space. If this is the case, see the section below on how to add more space.</p> </li> <li> <p>There is a known issue with the depot appliance when you deploy to a ESX host directly. In this case, the Jupyter Labs server may not be able to be configured. The depot will still function, regardless.  To work around this, deploy the appliance to a vCenter instance.</p> </li> </ul>"},{"location":"offline_depot/#how-to-expand-the-storage","title":"How to Expand the Storage","text":"<p>You might need to expand the filesystem of the depot as you populate it with more binaries. </p> <p>These procedures are documented in the included Jupyter Notebook. This allows you to execute them directly from there. However, if you chose not to install the Jupyter Notebooks, the instructions are duplicated here.</p> <p>First, you need to increase the size of the physical disk. To do this, simply edit the size of the disk for the VM and increase it to what will suit your needs. It will support up to 5.9TB in size.</p> <p>Next, we need to make sure the OS is aware of the space increase: <pre><code>echo 1 &gt; /sys/block/sda/device/rescan\n</code></pre> Finally, we will execute this command in order to resize the partition <pre><code>printf 'yes\\n100%%\\n' | parted /dev/sda resizepart 2 ---pretend-input-tty\n</code></pre> You can verify that the partition has been resized by using the following command: <pre><code>parted -s -a opt /dev/sda \"print free\"\n</code></pre></p> <p>Next, you need to expand the filesystem in order to take advantage of the new partition space by using a command like: <pre><code>resize2fs /dev/sda2\n</code></pre></p>"},{"location":"release_notes/","title":"Release Notes","text":"<p>This page documents the key features, enhancements, and capabilities available in Holodeck releases.</p>"},{"location":"release_notes/#holodeck-90-initial-release","title":"Holodeck 9.0 \u2013 Initial Release","text":"<p>Release Date: June 2025 Supported VCF Versions: VCF 9.0, VCF 5.2.x Minimum ESX Version: 8.0 U3</p>"},{"location":"release_notes/#whats-new","title":"What's New","text":"<ul> <li>Support for both VCF 9.0 and VCF 5.2 deployments from a single toolkit</li> <li>vSAN ESA and OSA deployment options</li> <li>New VVF deployment mode (VMware vSphere Foundation)</li> <li>Proxy support for online/offline depot workflows</li> <li>Enhanced PowerShell cmdlets with modular support:</li> <li><code>New-HoloDeckConfig</code>, <code>New-HoloDeckInstance</code>, <code>Start-HoloDeckInstance</code>, <code>Stop-HoloDeckInstance</code></li> <li>Option to deploy greenfield VCF environments with provision-only mode</li> </ul>"},{"location":"release_notes/#environment-breakdown","title":"Environment Breakdown","text":"<p>Each Holodeck environment includes a pre-configured set of virtual infrastructure components.</p> VCF 9.0VCF 5.2 <ul> <li>A Holorouter appliance (Photon OS based) with:</li> <li>DNS, DHCP, NTP, Proxy</li> <li>Dynamic routing (BGP), L2 switching</li> <li>Optional webtop (browser-based desktop)</li> <li>Support for VCF and VVF deployments</li> <li>Supports vSAN ESA and vSAN OSA</li> <li>Online and offline depot support for the VCF Installer</li> <li>Management Domain includes:</li> <li>4 nested ESX hosts as vSAN ready nodes</li> <li>VCF Installer, vCenter, NSX, VCF Operations, SDDC Manager</li> <li>Optional: VCF Automation</li> <li>Optional Workload Domain:</li> <li>3 nested ESX hosts</li> <li>vCenter, NSX, optional Supervisor Cluster</li> <li>Optional NSX Edge Clusters in management and/or workload domains</li> <li>Deploy additional 3-node vSphere clusters within management domain</li> <li>Provision-only mode to deploy just the installer and hosts</li> <li>Custom CIDR support for flexible networking</li> </ul> <ul> <li>A Holorouter appliance (Photon OS based) with:</li> <li>DNS, DHCP, NTP, Proxy</li> <li>Dynamic routing (BGP), L2 switching</li> <li>Optional webtop (browser-based desktop)</li> <li>Supports VCF deployment only</li> <li>vSAN OSA support</li> <li>Management Domain includes:</li> <li>4 nested ESX hosts as vSAN ready nodes</li> <li>Cloud Builder, vCenter, NSX, SDDC Manager</li> <li>Optional Workload Domain:</li> <li>3 nested ESX hosts</li> <li>vCenter and NSX</li> <li>Optional NSX Edge Clusters in management and/or workload domains</li> <li>Deploy additional 3-node vSphere clusters within management domain</li> <li>Custom CIDR support for flexible networking</li> </ul>"},{"location":"release_notes/#known-issues","title":"Known Issues","text":"<ul> <li>Hosts with memory tiering enabled may cause instability in nested workloads</li> <li>vSAN ESA may consume more storage than expected due to nested deduplication/compression behavior</li> <li> <p>Online Depot Check Failure:</p> <p>If you\u2019re using the online depot route for VCF Installer configuration, there is a known issue where the final validation step fails due to a mismatch between the API response format expected by Holodeck and the actual output from VCF Installer.</p> <pre><code>27-06-2025 00:43:32 SddcMgmtDomain[63248]: [INFO] Setting up depot for VCF Installer\n27-06-2025 00:43:32 SddcMgmtDomain[63248]: [INFO] Depot Type selected: online\n27-06-2025 00:43:34 SddcMgmtDomain[63248]: [ERROR] Depot connection failed.\nConvertFrom-Json: Cannot bind argument to parameter 'InputObject' because it is null.\nException: Exiting\n</code></pre> </li> </ul>"},{"location":"release_notes/#workaround","title":"Workaround","text":"<ul> <li>Login to the VCF Installer UI via Webtop   (typically at <code>https://10.1.10.250</code> unless a custom CIDR was used)</li> <li>Check if the depot is already configured successfully</li> <li>If confirmed, navigate back to PowerShell and run:</li> </ul> <pre><code>vi $config.state\n</code></pre> <ul> <li>Find and manually update the section:</li> </ul> <pre><code>{\n  \"VCF-Installer-Depot-Setup\": {\n    \"Status\": \"InProgress\"\n  }\n}\n</code></pre> <ul> <li>Change <code>\"Status\": \"InProgress\"</code> to <code>\"Status\": \"Success\"</code> to unblock the process.</li> <li>Run <code>New-HoloDeckInstance</code> command again to pick up from where it failed.</li> </ul> <p>Track this issue and future fix here: GitHub Issue #1</p>"},{"location":"release_notes/#previous-versions","title":"Previous Versions","text":"<p>Looking for Holodeck 5.2 setup guidance? Refer to the Holodeck 5.2 documentation.</p>"},{"location":"release_notes/#reporting-issues","title":"Reporting Issues","text":"<p>To raise issues, feature requests, or provide feedback, please visit the Holodeck GitHub repository.</p>"},{"location":"support_community/","title":"Support","text":"Support &amp; Feedback <p>If you're encountering issues, need help, or want to engage with the community, we offer two main channels of support.</p>"},{"location":"support_community/#report-bugs-or-request-features","title":"Report Bugs or Request Features","text":"<p>For all technical issues and feature requests related to Holodeck, please use our GitHub repository:</p> <p>vmware/Holodeck \u2013 GitHub Issues</p> <p>You can: - Report bugs or unexpected behavior - Suggest new features - Ask questions about setup, usage, or troubleshooting - Recommend improvements to the documentation</p> <p>Before submitting, please check if an issue already exists.</p>"},{"location":"support_community/#join-the-broader-community","title":"Join the Broader Community","text":"<p>For wider discussions, product insights, and community-contributed content, visit the Broadcom VCF Community:</p> <p>VMware Holodeck Community</p> <p>There you\u2019ll find: - Community blogs &amp; technical articles - Product announcements - Discussions with other VCF users &amp; contributors - Tips from Broadcom and VMware teams</p>"},{"location":"support_community/#contribute-to-holodeck","title":"Contribute to Holodeck","text":"<p>Want to contribute ideas, code, or documentation to Holodeck?</p> <p>github.com/vmware/Holodeck</p> <p>Together, we can make Holodeck better for everyone</p>"}]}